"""Gradio UI for Voice + MCP Inferencer
å®ç°è¯­éŸ³è¾“å…¥ + MCP å·¥å…·è°ƒç”¨çš„ Chatbot ç•Œé¢
è¾¹å½•è¾¹è½¬æ¨¡å¼ï¼šä½¿ç”¨åå°çº¿ç¨‹ + é˜Ÿåˆ—å®ç°å®æ—¶è½¬å†™
"""

import asyncio
import json
import os
import queue
import ssl
import threading
from dataclasses import dataclass, field
from typing import Generator, Optional

import gradio as gr
import httpx
import numpy as np
from openai import AsyncOpenAI

from dazhi.codec.audio import SAMPLE_RATE, encode_audio_to_base64
from dazhi.inferencers.mcp.inferencer import (
    LLMConfig,
    MCPClient,
    MCPConfig,
)


@dataclass
class SessionState:
    """æ¯ä¸ªç”¨æˆ·ä¼šè¯çš„çŠ¶æ€ - ä½¿ç”¨åå°çº¿ç¨‹ + é˜Ÿåˆ—æ¨¡å¼"""
    is_active: bool = False
    audio_queue: queue.Queue = field(default_factory=queue.Queue)
    results_queue: queue.Queue = field(default_factory=queue.Queue)
    llm_request_queue: queue.Queue = field(default_factory=queue.Queue)
    llm_response_queue: queue.Queue = field(default_factory=queue.Queue)
    stop_event: threading.Event = field(default_factory=threading.Event)
    worker_thread: Optional[threading.Thread] = None
    llm_worker_thread: Optional[threading.Thread] = None
    current_transcript: str = ""
    audio_buffer: list = field(default_factory=list)
    error_message: str = ""


class VoiceMCPGradioApp:
    """è¯­éŸ³ + MCP Gradio åº”ç”¨"""

    def __init__(
        self,
        mcp_config: MCPConfig,
        llm_config: LLMConfig | None = None,
    ):
        self.mcp_config = mcp_config
        self.llm_config = llm_config or LLMConfig()
        
        # å…±äº«çš„å®¢æˆ·ç«¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        self.mcp_client: MCPClient | None = None
        self.llm_client: AsyncOpenAI | None = None
        self.messages: list[dict] = []
        self.is_connected: bool = False

        # Realtime API é…ç½®
        self.realtime_base_url = os.getenv("OPENAI_BASE_URL")
        self.realtime_api_key = os.getenv("OPENAI_API_KEY")
        self.realtime_model = "transcribe"

    def _run_async(self, coro):
        """è¿è¡Œå¼‚æ­¥åç¨‹"""
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)

    def connect(self) -> str:
        """è¿æ¥ MCP å’Œ LLM æœåŠ¡"""
        async def _connect():
            try:
                # åˆå§‹åŒ– MCP å®¢æˆ·ç«¯
                self.mcp_client = MCPClient(self.mcp_config)
                await self.mcp_client.connect()

                # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
                self.llm_client = AsyncOpenAI(
                    base_url=self.llm_config.base_url,
                    api_key=self.llm_config.api_key,
                    http_client=httpx.AsyncClient(verify=False),
                )

                # åˆå§‹åŒ–æ¶ˆæ¯å†å²
                self.messages = [
                    {"role": "system", "content": self.llm_config.system_prompt}
                ]

                self.is_connected = True
                tools = [t.name for t in self.mcp_client.tools]
                return f"âœ… è¿æ¥æˆåŠŸï¼\nå¯ç”¨å·¥å…·: {', '.join(tools)}"

            except Exception as e:
                self.is_connected = False
                return f"âŒ è¿æ¥å¤±è´¥: {e}"
        
        return self._run_async(_connect())

    def disconnect(self) -> str:
        """æ–­å¼€è¿æ¥"""
        async def _disconnect():
            try:
                if self.mcp_client:
                    await self.mcp_client.disconnect()
                self.is_connected = False
                self.mcp_client = None
                self.llm_client = None
                return "å·²æ–­å¼€è¿æ¥"
            except Exception as e:
                return f"æ–­å¼€è¿æ¥æ—¶å‡ºé”™: {e}"
        
        return self._run_async(_disconnect())

    def process_text(
        self,
        message: str,
        history: list,
    ) -> Generator:
        """å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆæµå¼è¾“å‡ºï¼‰"""
        if not message.strip():
            yield history
            return

        if not self.is_connected:
            history = history + [{"role": "assistant", "content": "âŒ è¯·å…ˆç‚¹å‡»ã€Œè¿æ¥æœåŠ¡ã€"}]
            yield history
            return

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        history = history + [{"role": "user", "content": message}]
        yield history

        # æ·»åŠ åˆ°å†…éƒ¨æ¶ˆæ¯åˆ—è¡¨ï¼ˆåŠ  /no_think å…³é—­æ€è€ƒï¼‰
        self.messages.append({"role": "user", "content": message + " /no_think"})

        # è·å– MCP å·¥å…·
        tools = self.mcp_client.get_tools_for_openai() if self.mcp_client else []

        async def _process():
            try:
                # è°ƒç”¨ LLMï¼ˆæµå¼ï¼‰
                response = await self.llm_client.chat.completions.create(
                    model=self.llm_config.model,
                    messages=self.messages,
                    tools=tools if tools else None,
                    stream=True,
                )

                content_chunks = []
                tool_calls = []

                async for chunk in response:
                    if not chunk.choices:
                        continue
                    delta = chunk.choices[0].delta
                    if delta is None:
                        continue

                    if delta.content:
                        content_chunks.append(delta.content)

                    if delta.tool_calls:
                        for tc in delta.tool_calls:
                            while len(tool_calls) <= tc.index:
                                tool_calls.append({
                                    "id": None,
                                    "type": "function",
                                    "function": {"name": None, "arguments": ""},
                                })
                            target = tool_calls[tc.index]
                            if tc.id:
                                target["id"] = tc.id
                            if tc.function and tc.function.name:
                                target["function"]["name"] = tc.function.name
                            if tc.function and tc.function.arguments:
                                target["function"]["arguments"] += tc.function.arguments

                return "".join(content_chunks), tool_calls

            except Exception as e:
                return f"âŒ å‡ºé”™: {e}", []

        content, tool_calls = self._run_async(_process())
        
        # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
        assistant_msg = {"role": "assistant", "content": content}
        if tool_calls:
            assistant_msg["tool_calls"] = tool_calls
        self.messages.append(assistant_msg)

        # æ˜¾ç¤ºåŠ©æ‰‹å›å¤
        history = history + [{"role": "assistant", "content": content}]
        yield history

        # å¤„ç†å·¥å…·è°ƒç”¨
        if tool_calls:
            for tc in tool_calls:
                tool_name = tc["function"]["name"]
                tool_args_str = tc["function"]["arguments"]
                tool_args = json.loads(tool_args_str) if tool_args_str else {}

                # æ˜¾ç¤ºå·¥å…·è°ƒç”¨
                history[-1]["content"] += f"\n\nğŸ”§ **è°ƒç”¨å·¥å…·**: \`{tool_name}\`"
                yield history

                # æ‰§è¡Œå·¥å…·
                async def _call_tool():
                    try:
                        result = await self.mcp_client.call_tool(tool_name, tool_args)
                        return result
                    except Exception as e:
                        return f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"

                result = self._run_async(_call_tool())
                result_preview = result[:300] + "..." if len(result) > 300 else result
                history[-1]["content"] += f"\nğŸ“‹ **ç»“æœ**: {result_preview}"
                yield history

                self.messages.append({
                    "role": "tool",
                    "tool_call_id": tc["id"],
                    "content": result,
                })

            # å†æ¬¡è°ƒç”¨ LLM
            async def _final_response():
                response = await self.llm_client.chat.completions.create(
                    model=self.llm_config.model,
                    messages=self.messages,
                    stream=True,
                )
                chunks = []
                async for chunk in response:
                    if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                        chunks.append(chunk.choices[0].delta.content)
                return "".join(chunks)

            final_content = self._run_async(_final_response())
            self.messages.append({"role": "assistant", "content": final_content})
            history = history + [{"role": "assistant", "content": final_content}]
            yield history

    def _websocket_worker(self, state: SessionState):
        """åå°çº¿ç¨‹ï¼šç®¡ç† WebSocket è¿æ¥ï¼Œå‘é€éŸ³é¢‘å¹¶æ¥æ”¶è½¬å†™ç»“æœ"""
        
        async def _run():
            try:
                # åˆ›å»º Realtime å®¢æˆ·ç«¯
                client = AsyncOpenAI(
                    base_url=self.realtime_base_url,
                    api_key=self.realtime_api_key,
                )
                
                ssl_context = ssl.create_default_context()
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
                
                async with client.realtime.connect(
                    model=self.realtime_model,
                    websocket_connection_options={"ssl": ssl_context},
                ) as conn:
                    # é…ç½®ä¼šè¯
                    await conn.session.update(
                        session={
                            "output_modalities": ["text"],
                        }
                    )
                    print("[DEBUG] WebSocket è¿æ¥å·²å»ºç«‹ï¼ŒSession å·²é…ç½®")
                    state.results_queue.put(("connected", None))
                    
                    async def send_audio():
                        """ä»é˜Ÿåˆ—è¯»å–éŸ³é¢‘å¹¶å‘é€ - æœåŠ¡ç«¯è‡ªåŠ¨ VAD"""
                        while not state.stop_event.is_set():
                            try:
                                audio_base64 = state.audio_queue.get(timeout=0.1)
                                if audio_base64 == "STOP_SESSION":
                                    print("[DEBUG] æ”¶åˆ°åœæ­¢ä¿¡å·")
                                    break
                                await conn.input_audio_buffer.append(audio=audio_base64)
                            except queue.Empty:
                                await asyncio.sleep(0.01)
                            except Exception as e:
                                print(f"[DEBUG] å‘é€éŸ³é¢‘é”™è¯¯: {e}")
                                break
                    
                    async def receive_events():
                        """æ¥æ”¶è½¬å†™äº‹ä»¶ - æŒç»­æ¨¡å¼"""
                        try:
                            async for event in conn:
                                if state.stop_event.is_set():
                                    break
                                    
                                event_type = event.type
                                
                                if event_type == "response.output_audio_transcript.delta":
                                    # æ¯ä¸ª delta éƒ½æ˜¯å®Œæ•´å¯ç”¨çš„ç»“æœï¼ˆæœåŠ¡ç«¯ VAD åˆ‡åˆ†ï¼‰
                                    transcript = event.delta
                                    print(f"[DEBUG] æ”¶åˆ°è½¬å†™ç»“æœ: {transcript}")
                                    state.results_queue.put(("transcript", transcript))
                                    
                                elif event_type == "response.done":
                                    print("[DEBUG] å“åº”å®Œæˆï¼Œç­‰å¾…ä¸‹ä¸€ä¸ªéŸ³é¢‘å—...")
                                    state.results_queue.put(("done", None))
                                    # ä¸ breakï¼Œç»§ç»­ç­‰å¾…ä¸‹ä¸€ä¸ªéŸ³é¢‘å—çš„è½¬å†™
                                    
                                elif event_type == "error":
                                    print(f"[DEBUG] é”™è¯¯äº‹ä»¶: {event}")
                                    state.results_queue.put(("error", str(event)))
                                    # é”™è¯¯æ—¶ä¹Ÿä¸é€€å‡ºï¼Œè®©ä¸Šå±‚å¤„ç†
                        except Exception as e:
                            print(f"[DEBUG] æ¥æ”¶äº‹ä»¶é”™è¯¯: {e}")
                            state.results_queue.put(("error", str(e)))
                    
                    # å¹¶è¡Œè¿è¡Œå‘é€å’Œæ¥æ”¶
                    await asyncio.gather(
                        send_audio(),
                        receive_events(),
                        return_exceptions=True,
                    )
                    
            except Exception as e:
                print(f"[DEBUG] WebSocket worker é”™è¯¯: {e}")
                state.results_queue.put(("error", str(e)))
            finally:
                state.results_queue.put(("closed", None))
                state.is_active = False
                print("[DEBUG] WebSocket worker ç»“æŸ")
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
        asyncio.run(_run())

    def _llm_worker(self, state: SessionState):
        """åå°çº¿ç¨‹ï¼šå¤„ç† LLM è¯·æ±‚"""
        while not state.stop_event.is_set():
            try:
                # è·å–è½¬å†™ç»“æœ
                transcript = state.llm_request_queue.get(timeout=0.5)
                if transcript == "STOP":
                    break
                    
                print(f"[DEBUG] LLM worker æ”¶åˆ°è¯·æ±‚: '{transcript}'")
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†…éƒ¨æ¶ˆæ¯åˆ—è¡¨
                self.messages.append({"role": "user", "content": transcript + " /no_think"})
                
                # è·å– MCP å·¥å…·
                tools = self.mcp_client.get_tools_for_openai() if self.mcp_client else []
                
                # è°ƒç”¨ LLM
                async def _call_llm():
                    try:
                        response = await self.llm_client.chat.completions.create(
                            model=self.llm_config.model,
                            messages=self.messages,
                            tools=tools if tools else None,
                            stream=True,
                        )
                        
                        content_chunks = []
                        tool_calls = []
                        
                        async for chunk in response:
                            if not chunk.choices:
                                continue
                            delta = chunk.choices[0].delta
                            if delta is None:
                                continue
                            if delta.content:
                                content_chunks.append(delta.content)
                            if delta.tool_calls:
                                for tc in delta.tool_calls:
                                    while len(tool_calls) <= tc.index:
                                        tool_calls.append({
                                            "id": None,
                                            "type": "function",
                                            "function": {"name": None, "arguments": ""},
                                        })
                                    target = tool_calls[tc.index]
                                    if tc.id:
                                        target["id"] = tc.id
                                    if tc.function and tc.function.name:
                                        target["function"]["name"] = tc.function.name
                                    if tc.function and tc.function.arguments:
                                        target["function"]["arguments"] += tc.function.arguments
                        
                        return "".join(content_chunks), tool_calls
                    except Exception as e:
                        return f"âŒ LLM å‡ºé”™: {e}", []
                
                content, tool_calls = self._run_async(_call_llm())
                
                # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
                assistant_msg = {"role": "assistant", "content": content}
                if tool_calls:
                    assistant_msg["tool_calls"] = tool_calls
                self.messages.append(assistant_msg)
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                if tool_calls:
                    for tc in tool_calls:
                        tool_name = tc["function"]["name"]
                        tool_args_str = tc["function"]["arguments"]
                        import json
                        tool_args = json.loads(tool_args_str) if tool_args_str else {}
                        
                        async def _call_tool():
                            try:
                                result = await self.mcp_client.call_tool(tool_name, tool_args)
                                return result
                            except Exception as e:
                                return f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
                        
                        result = self._run_async(_call_tool())
                        content += f"\n\nğŸ”§ **è°ƒç”¨å·¥å…·**: `{tool_name}`\nğŸ“‹ **ç»“æœ**: {result[:300]}..."
                        
                        self.messages.append({
                            "role": "tool",
                            "tool_call_id": tc["id"],
                            "content": result,
                        })
                    
                    # å†æ¬¡è°ƒç”¨ LLM è·å–æœ€ç»ˆå›å¤
                    async def _final_response():
                        response = await self.llm_client.chat.completions.create(
                            model=self.llm_config.model,
                            messages=self.messages,
                            stream=True,
                        )
                        chunks = []
                        async for chunk in response:
                            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                                chunks.append(chunk.choices[0].delta.content)
                        return "".join(chunks)
                    
                    final_content = self._run_async(_final_response())
                    self.messages.append({"role": "assistant", "content": final_content})
                    content = final_content
                
                # å°†ç»“æœæ”¾å…¥å“åº”é˜Ÿåˆ—
                state.llm_response_queue.put(("response", transcript, content))
                print(f"[DEBUG] LLM worker å®Œæˆå¤„ç†")
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[DEBUG] LLM worker é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

    def _start_session(self, state: SessionState | None) -> tuple[SessionState, Optional[str]]:
        """å¯åŠ¨æ–°çš„è½¬å†™ä¼šè¯"""
        # åœæ­¢æ—§ä¼šè¯
        if state is not None and state.is_active:
            state.stop_event.set()
            state.audio_queue.put("STOP_SESSION")
            state.llm_request_queue.put("STOP")
        
        # åˆ›å»ºæ–°ä¼šè¯
        state = SessionState(is_active=True)
        
        # å¯åŠ¨ WebSocket worker çº¿ç¨‹
        state.worker_thread = threading.Thread(
            target=self._websocket_worker,
            args=(state,),
            daemon=True,
        )
        state.worker_thread.start()
        
        # å¯åŠ¨ LLM worker çº¿ç¨‹
        state.llm_worker_thread = threading.Thread(
            target=self._llm_worker,
            args=(state,),
            daemon=True,
        )
        state.llm_worker_thread.start()
        
        # ç­‰å¾…è¿æ¥å»ºç«‹
        import time
        time.sleep(0.3)
        
        try:
            event_type, payload = state.results_queue.get_nowait()
            if event_type == "error":
                state.is_active = False
                return state, f"è¿æ¥å¤±è´¥: {payload}"
            if event_type == "connected":
                print("[DEBUG] ä¼šè¯å·²å¯åŠ¨")
                return state, None
            # æ”¾å›é˜Ÿåˆ—
            state.results_queue.put((event_type, payload))
        except queue.Empty:
            pass
        
        return state, None

    def process_audio_stream(
        self,
        audio_chunk: tuple[int, np.ndarray] | tuple | None,
        history: list,
        state: SessionState | None,
    ) -> tuple[str, list, SessionState | None]:
        """å¤„ç†æµå¼éŸ³é¢‘ - è¾¹å½•è¾¹è½¬"""
        
        # å¤„ç†ç©ºéŸ³é¢‘æ•°æ®
        if audio_chunk is None or len(audio_chunk) == 0:
            if state is not None and state.current_transcript:
                return f"ğŸ¤ {state.current_transcript}", gr.update(), state
            return "ç­‰å¾…è¯­éŸ³è¾“å…¥...", gr.update(), state

        if not self.is_connected:
            return "âŒ è¯·å…ˆè¿æ¥æœåŠ¡", gr.update(), state

        # è§£åŒ…éŸ³é¢‘æ•°æ®
        if len(audio_chunk) != 2:
            return "éŸ³é¢‘æ ¼å¼é”™è¯¯", gr.update(), state
            
        sample_rate, audio_array = audio_chunk

        try:
            # è½¬æ¢éŸ³é¢‘æ ¼å¼
            if len(audio_array.shape) > 1:
                audio_array = audio_array.mean(axis=1)
            if audio_array.dtype != np.int16:
                if np.issubdtype(audio_array.dtype, np.floating):
                    audio_array = (audio_array * 32767).astype(np.int16)
                else:
                    audio_array = audio_array.astype(np.int16)

            # é‡é‡‡æ ·åˆ° 16000 Hz
            if sample_rate != SAMPLE_RATE:
                ratio = SAMPLE_RATE / sample_rate
                new_length = int(len(audio_array) * ratio)
                indices = np.linspace(0, len(audio_array) - 1, new_length).astype(int)
                audio_array = audio_array[indices].astype(np.int16)

            # é¦–æ¬¡è°ƒç”¨æ—¶å¯åŠ¨ä¼šè¯
            if state is None or not state.is_active:
                state, error = self._start_session(state)
                if error:
                    return f"âŒ {error}", history, state

            # å‘é€éŸ³é¢‘åˆ°é˜Ÿåˆ—ï¼ˆæœåŠ¡ç«¯è‡ªåŠ¨ VADï¼‰
            audio_base64 = encode_audio_to_base64(audio_array.tobytes())
            state.audio_queue.put(audio_base64)
            state.audio_buffer.append(audio_array)

            # æ£€æŸ¥è½¬å†™ç»“æœ - æ¯ä¸ª transcript delta éƒ½æ˜¯å®Œæ•´å—ï¼ˆæœåŠ¡ç«¯ VADï¼‰
            history_updated = False
            while True:
                try:
                    event_type, payload = state.results_queue.get_nowait()
                    if event_type == "transcript":
                        # æœåŠ¡ç«¯ VAD åˆ‡åˆ†ï¼Œæ¯ä¸ª delta éƒ½æ˜¯å®Œæ•´å¥å­
                        transcript = payload.strip()
                        if transcript:
                            state.current_transcript = transcript
                            print(f"[DEBUG] æ”¶åˆ°å®Œæ•´å—ï¼Œå‘é€ç»™ LLM: '{transcript}'")
                            
                            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ° history
                            history = history + [{"role": "user", "content": transcript}]
                            history_updated = True
                            
                            # å‘é€ç»™ LLM worker å¤„ç†
                            state.llm_request_queue.put(transcript)
                        
                    elif event_type == "error":
                        state.error_message = payload
                    elif event_type == "done":
                        # å“åº”å®Œæˆï¼Œä¸åšä»»ä½•äº‹ï¼Œç­‰å¾…æ–°éŸ³é¢‘
                        print("[DEBUG] å“åº”å®Œæˆ")
                    elif event_type == "closed":
                        state.is_active = False
                except queue.Empty:
                    break
            
            # æ£€æŸ¥ LLM å“åº”
            while True:
                try:
                    event_type, user_msg, assistant_msg = state.llm_response_queue.get_nowait()
                    if event_type == "response":
                        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ° history
                        history = history + [{"role": "assistant", "content": assistant_msg}]
                        history_updated = True
                        print(f"[DEBUG] LLM å›å¤å·²æ·»åŠ åˆ° history")
                except queue.Empty:
                    break

            # è¿”å›å½“å‰çŠ¶æ€ - åªæœ‰åœ¨ history æœ‰å˜åŒ–æ—¶æ‰æ›´æ–° chatbot
            status_msg = "ğŸ¤ æ­£åœ¨å½•éŸ³..."
            if state.error_message:
                status_msg = f"âŒ {state.error_message}"
            elif state.current_transcript:
                status_msg = f"ğŸ¤ æœ€æ–°è½¬å†™: {state.current_transcript}"
            
            if history_updated:
                return status_msg, history, state
            else:
                # ä½¿ç”¨ gr.update() è·³è¿‡ chatbot æ›´æ–°ä»¥é¿å…é—ªçƒ
                return status_msg, gr.update(), state

        except Exception as e:
            print(f"[DEBUG] æµå¼éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            return f"âŒ éŸ³é¢‘å¤„ç†é”™è¯¯: {e}", gr.update(), state

    def stop_and_process(
        self,
        history: list,
        state: SessionState | None,
    ) -> Generator[tuple[str, list, SessionState | None], None, None]:
        """åœæ­¢å½•éŸ³å¹¶å¤„ç†æœ€åçš„è½¬å†™ç»“æœ"""
        print(f"[DEBUG] stop_and_process è¢«è°ƒç”¨")
        
        if state is None:
            yield "æ²¡æœ‰æ´»åŠ¨çš„å½•éŸ³ä¼šè¯", history, None
            return

        try:
            # åœæ­¢ä¼šè¯
            state.stop_event.set()
            state.audio_queue.put("STOP_SESSION")  # å‘é€åœæ­¢ä¿¡å·ç»™ WebSocket worker
            state.llm_request_queue.put("STOP")
            state.is_active = False
            
            # ç­‰å¾…å‰©ä½™çš„ LLM å“åº”
            import time
            time.sleep(0.5)
            
            # æ”¶é›†å‰©ä½™çš„ LLM å“åº”
            while True:
                try:
                    event_type, user_msg, assistant_msg = state.llm_response_queue.get_nowait()
                    if event_type == "response":
                        history = history + [{"role": "assistant", "content": assistant_msg}]
                except queue.Empty:
                    break
            
            yield "âœ… å½•éŸ³å·²åœæ­¢", history, None

        except Exception as e:
            import traceback
            traceback.print_exc()
            print(f"[DEBUG] å¤„ç†é”™è¯¯: {e}")
            yield f"âŒ å¤„ç†å¤±è´¥: {e}", history, None

    def clear_chat(self) -> tuple[list, str]:
        """æ¸…ç©ºå¯¹è¯"""
        self.messages = [
            {"role": "system", "content": self.llm_config.system_prompt}
        ]
        return [], ""

    def build_ui(self) -> gr.Blocks:
        """æ„å»º Gradio UI"""
        with gr.Blocks(
            title="ğŸ” è¯­éŸ³ + MCP æ™ºèƒ½åŠ©æ‰‹",
            theme=gr.themes.Soft(),
        ) as demo:
            gr.Markdown("""
            # ğŸ” è¯­éŸ³ + MCP æ™ºèƒ½åŠ©æ‰‹
            
            æ”¯æŒ **è¯­éŸ³è¾“å…¥** å’Œ **æ–‡æœ¬è¾“å…¥**ï¼Œå¯è°ƒç”¨ MCP å·¥å…·å®Œæˆä»»åŠ¡ã€‚
            
            **ä½¿ç”¨æ–¹æ³•**ï¼šç‚¹å‡»éº¦å…‹é£å¼€å§‹å½•éŸ³ï¼Œå®æ—¶æ˜¾ç¤ºè½¬å†™ç»“æœï¼Œè¯´å®Œåç‚¹å‡»ã€Œåœæ­¢å¹¶å‘é€ã€ã€‚
            """)

            # ä¼šè¯çŠ¶æ€
            session_state = gr.State(value=None)

            with gr.Row():
                # å·¦ä¾§æ§åˆ¶é¢æ¿
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ è¿æ¥è®¾ç½®")
                    with gr.Group():
                        connect_btn = gr.Button("ğŸ”Œ è¿æ¥æœåŠ¡", variant="primary", size="lg")
                        disconnect_btn = gr.Button("æ–­å¼€è¿æ¥", variant="secondary")
                        status_box = gr.Textbox(
                            label="è¿æ¥çŠ¶æ€",
                            value="æœªè¿æ¥",
                            interactive=False,
                            lines=3,
                        )

                    gr.Markdown("### ğŸ¤ è¯­éŸ³è¾“å…¥ï¼ˆå®æ—¶è½¬å†™ï¼‰")
                    with gr.Group():
                        audio_input = gr.Audio(
                            sources=["microphone"],
                            type="numpy",
                            label="ç‚¹å‡»éº¦å…‹é£å¼€å§‹å½•éŸ³ï¼ˆè‡ªåŠ¨è¿æ¥ï¼‰",
                            streaming=True,
                        )
                        stop_btn = gr.Button("ğŸ›‘ åœæ­¢å¹¶å‘é€", variant="primary")
                        voice_status = gr.Textbox(
                            label="å®æ—¶è½¬å†™",
                            value="ç‚¹å‡»éº¦å…‹é£å¼€å§‹å½•éŸ³...",
                            interactive=False,
                            lines=2,
                        )

                # å³ä¾§èŠå¤©åŒºåŸŸ
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯",
                        height=450,
                    )

                    with gr.Row():
                        text_input = gr.Textbox(
                            label="æ–‡æœ¬è¾“å…¥",
                            placeholder="è¾“å…¥æ¶ˆæ¯æˆ–ä½¿ç”¨è¯­éŸ³...",
                            scale=4,
                            lines=1,
                        )
                        send_btn = gr.Button("å‘é€", variant="primary", scale=1)

                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯")

            # ========== äº‹ä»¶ç»‘å®š ==========

            # è¿æ¥/æ–­å¼€
            connect_btn.click(
                fn=self.connect,
                outputs=[status_box],
            )
            disconnect_btn.click(
                fn=self.disconnect,
                outputs=[status_box],
            )

            # æ–‡æœ¬è¾“å…¥
            text_input.submit(
                fn=self.process_text,
                inputs=[text_input, chatbot],
                outputs=[chatbot],
            ).then(
                fn=lambda: "",
                outputs=[text_input],
            )

            send_btn.click(
                fn=self.process_text,
                inputs=[text_input, chatbot],
                outputs=[chatbot],
            ).then(
                fn=lambda: "",
                outputs=[text_input],
            )

            # æµå¼è¯­éŸ³è¾“å…¥ - è¾¹å½•è¾¹è½¬ï¼ˆåªæ›´æ–°çŠ¶æ€ï¼Œä¸ç›´æ¥æ›´æ–° chatbotï¼‰
            stream_event = audio_input.stream(
                fn=self.process_audio_stream,
                inputs=[audio_input, chatbot, session_state],
                outputs=[voice_status, chatbot, session_state],
                stream_every=0.5,  # 0.5ç§’æ£€æŸ¥ä¸€æ¬¡
                time_limit=60,
                concurrency_limit=1,
            )

            # åœæ­¢å½•éŸ³å¹¶å¤„ç†
            stop_btn.click(
                fn=self.stop_and_process,
                inputs=[chatbot, session_state],
                outputs=[voice_status, chatbot, session_state],
                cancels=[stream_event],
            )

            # æ¸…ç©ºå¯¹è¯
            clear_btn.click(
                fn=self.clear_chat,
                outputs=[chatbot, text_input],
            )

        return demo

    def launch(self, **kwargs):
        """å¯åŠ¨åº”ç”¨"""
        demo = self.build_ui()
        demo.queue()
        demo.launch(**kwargs)


def create_app(
    mcp_url: str,
    mcp_token: str | None = None,
    llm_model: str = "qwen3:8B",
    system_prompt: str | None = None,
) -> VoiceMCPGradioApp:
    """åˆ›å»ºåº”ç”¨å®ä¾‹"""
    mcp_config = MCPConfig(mcp_url=mcp_url, mcp_token=mcp_token)
    llm_config = LLMConfig(
        model=llm_model,
        system_prompt=system_prompt or "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
    )

    return VoiceMCPGradioApp(
        mcp_config=mcp_config,
        llm_config=llm_config,
    )


if __name__ == "__main__":
    import dotenv

    dotenv.load_dotenv()

    app = create_app(
        mcp_url="https://mcp.mcd.cn/mcp-servers/mcd-mcp",
        llm_model="qwen3:8B",
        system_prompt="""ä½ æ˜¯éº¦å½“åŠ³æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·ï¼š
- æŸ¥è¯¢å½“å‰æ—¶é—´ (now-time-info)
- æŸ¥è¯¢æ´»åŠ¨æ—¥å† (campaign-calender)
- æŸ¥è¯¢å¯ç”¨ä¼˜æƒ åˆ¸ (available-coupons)
- ä»¥åŠå…¶ä»–éº¦å½“åŠ³ç›¸å…³æœåŠ¡

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç®€æ´æ˜äº†ã€‚å½“éœ€è¦æŸ¥è¯¢ä¿¡æ¯æ—¶ï¼Œè¯·è°ƒç”¨ç›¸åº”çš„å·¥å…·ã€‚""",
    )
    app.launch(share=False, server_name="0.0.0.0", server_port=7860)

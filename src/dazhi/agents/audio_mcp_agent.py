"""è¯­éŸ³ MCP Agent - ç»“åˆè¯­éŸ³è¯†åˆ«ã€LLM æŽ¨ç†ä¸Ž MCP å·¥å…·è°ƒç”¨"""

import asyncio
import json

from dazhi.handlers.voice_mcp import VoiceMCPEventHandler
from dazhi.inferencers.llm.inferencer import LLMInferencer
from dazhi.inferencers.llm.session import LLMChatSession
from dazhi.inferencers.realtime.inferencer import RealtimeConfig, RealtimeInferencer
from dazhi.mcp_adaptors.config import LLMConfig, MCPConfig
from dazhi.mcp_adaptors.mcp_client import MCPClient


class VoiceMCPAgent:
    """è¯­éŸ³ + MCP æŽ¨ç†å™¨

    ç»“åˆå®žæ—¶è¯­éŸ³è¯†åˆ«å’Œ MCP å·¥å…·è°ƒç”¨ï¼Œå®žçŽ°è¯­éŸ³æŽ§åˆ¶çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

    Example:
        ```python
        mcp_config = MCPConfig(mcp_url="https://mcp.mcd.cn/mcp-servers/mcd-mcp")
        agent = VoiceMCPAgent(mcp_config=mcp_config)
        await agent.run()
        ```
    """

    def __init__(
        self,
        mcp_config: MCPConfig,
        llm_config: LLMConfig | None = None,
        realtime_config: RealtimeConfig | None = None,
    ):
        self.mcp_config = mcp_config
        self.llm_config = llm_config or LLMConfig()
        self.realtime_config = realtime_config or RealtimeConfig()

        self.mcp_client = MCPClient(mcp_config)
        self.llm_inferencer = LLMInferencer(llm_config)
        self.realtime_inferencer: RealtimeInferencer | None = None
        self.llm_session: LLMChatSession | None = None

        self.is_running = False
        self._transcript_queue: asyncio.Queue[str] = asyncio.Queue()

    async def _process_transcripts(self) -> None:
        """å¤„ç†è¯­éŸ³è½¬å†™ç»“æžœé˜Ÿåˆ—"""
        while self.is_running:
            try:
                # ç­‰å¾…è¯­éŸ³è¾“å…¥
                transcript = await asyncio.wait_for(
                    self._transcript_queue.get(), timeout=0.5
                )

                # æ£€æŸ¥æ˜¯å¦é€€å‡º
                if transcript in ["é€€å‡º", "ç»“æŸ", "åœæ­¢", "exit", "quit"]:
                    print("\nðŸ‘‹ æ”¶åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œæ­£åœ¨åœæ­¢...")
                    self.is_running = False
                    break

                # å¤„ç†è¯­éŸ³è¾“å…¥
                tools = self.mcp_client.get_tools_for_openai()
                if not self.llm_session:
                    raise RuntimeError("LLM session æœªåˆå§‹åŒ–")

                assistant_message = await self.llm_inferencer.process_user_input(
                    self.llm_session, transcript, tools=tools
                )

                tool_calls = assistant_message.get("tool_calls", [])
                if tool_calls:
                    await self._handle_tool_calls(tool_calls)
                    await self.llm_inferencer.continue_assistant(self.llm_session)

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"\nâŒ å¤„ç†è½¬å†™ç»“æžœå‡ºé”™: {e}")

    async def _handle_tool_calls(self, tool_calls: list[dict]) -> None:
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_args = (
                json.loads(tool_call["function"]["arguments"])
                if tool_call["function"]["arguments"]
                else {}
            )

            print(f"ðŸ”§ è°ƒç”¨å·¥å…·: {tool_name}({tool_args})")

            try:
                result = await self.mcp_client.call_tool(tool_name, tool_args)
                print(
                    f"ðŸ“‹ å·¥å…·ç»“æžœ: {result[:200]}..."
                    if len(result) > 200
                    else f"ðŸ“‹ å·¥å…·ç»“æžœ: {result}"
                )
            except Exception as e:
                result = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
                print(f"âŒ {result}")

            if self.llm_session:
                self.llm_session.add_tool_result(tool_call["id"], result)

    async def run(self) -> None:
        """è¿è¡Œè¯­éŸ³ MCP æ¨¡å¼"""
        print("=" * 50)
        print("ðŸ” éº¦å½“åŠ³ MCP æ™ºèƒ½åŠ©æ‰‹ (è¯­éŸ³æ¨¡å¼)")
        print("=" * 50)

        self.is_running = True

        # åˆå§‹åŒ– MCP æŽ¨ç†å™¨
        await self.llm_inferencer._init_llm_client()
        await self.mcp_client.connect()
        self.llm_session = LLMChatSession(
            system_prompt=self.llm_inferencer.llm_config.system_prompt
        )
        self.llm_inferencer.is_running = True

        print("\nðŸŽ¤ è¯­éŸ³æ¨¡å¼å¯åŠ¨ï¼Œè¯·è¯´è¯...")
        print("   è¯´ 'é€€å‡º' æˆ– 'ç»“æŸ' åœæ­¢\n")

        # åˆ›å»ºäº‹ä»¶å¤„ç†å™¨
        event_handler = VoiceMCPEventHandler(self._transcript_queue)

        # åˆ›å»ºå®žæ—¶æŽ¨ç†å™¨
        self.realtime_inferencer = RealtimeInferencer(
            config=self.realtime_config,
            event_handler=event_handler,
        )

        try:
            # å¹¶è¡Œè¿è¡Œè¯­éŸ³è¯†åˆ«å’Œè½¬å†™å¤„ç†
            await asyncio.gather(
                self.realtime_inferencer.run(enable_audio_playback=False),
                self._process_transcripts(),
            )
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        finally:
            self.is_running = False
            if self.realtime_inferencer:
                await self.realtime_inferencer.stop()
            await self.mcp_client.disconnect()
            print("âœ“ æ‰€æœ‰è¿žæŽ¥å·²å…³é—­")

    async def __aenter__(self) -> "VoiceMCPAgent":
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        self.is_running = False
        if self.realtime_inferencer:
            await self.realtime_inferencer.stop()
        await self.mcp_client.disconnect()

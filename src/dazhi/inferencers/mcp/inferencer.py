"""MCP æ¨ç†å™¨æ¨¡å— - ç»“åˆ LLM å’Œ MCP å·¥å…·è°ƒç”¨"""

import asyncio
import json
import os
from dataclasses import dataclass, field
from typing import Any

import httpx
from mcp import ClientSession, types
from mcp.client.streamable_http import streamablehttp_client
from openai import AsyncOpenAI
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam

from dazhi.inferencers.realtime.inferencer import (
    RealtimeConfig,
    RealtimeEventHandler,
    RealtimeInferencer,
    TranscriptEvent,
    AudioEvent,
    InputTranscriptEvent,
)


@dataclass
class MCPConfig:
    """MCP é…ç½®"""

    mcp_url: str
    mcp_token: str | None = None
    headers: dict[str, str] = field(default_factory=dict)

    def __post_init__(self):
        if self.mcp_token is None:
            self.mcp_token = os.getenv("MCD_MCP_TOKEN", "").strip()
        if self.mcp_token and "Authorization" not in self.headers:
            self.headers["Authorization"] = f"Bearer {self.mcp_token}"


@dataclass
class LLMConfig:
    """LLM é…ç½®"""

    base_url: str | None = None
    api_key: str | None = None
    model: str = "qwen3:8B"
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢éº¦å½“åŠ³ç›¸å…³ä¿¡æ¯ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"

    def __post_init__(self):
        if self.base_url is None:
            self.base_url = os.getenv("OPENAI_BASE_URL", "http://localhost:10002/v1")
        if self.api_key is None:
            self.api_key = os.getenv("TEST_API_KEY", "dummy_api_key")


class MCPClient:
    """MCP å®¢æˆ·ç«¯å°è£…"""

    def __init__(self, config: MCPConfig):
        self.config = config
        self.session: ClientSession | None = None
        self.tools: list[types.Tool] = []
        self._read = None
        self._write = None
        self._cm = None

    async def connect(self) -> None:
        """è¿æ¥åˆ° MCP æœåŠ¡"""
        self._cm = streamablehttp_client(self.config.mcp_url, headers=self.config.headers)
        self._read, self._write, _ = await self._cm.__aenter__()
        self.session = ClientSession(self._read, self._write)
        await self.session.__aenter__()
        await self.session.initialize()

        # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
        tools_resp = await self.session.list_tools()
        self.tools = tools_resp.tools
        print(f"âœ“ MCP connected, available tools: {[t.name for t in self.tools]}")

    async def disconnect(self) -> None:
        """æ–­å¼€ MCP è¿æ¥"""
        if self.session:
            await self.session.__aexit__(None, None, None)
        if self._cm:
            await self._cm.__aexit__(None, None, None)

    def get_tools_for_openai(self) -> list[ChatCompletionToolParam]:
        """å°† MCP å·¥å…·è½¬æ¢ä¸º OpenAI æ ¼å¼"""
        openai_tools = []
        for tool in self.tools:
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description or "",
                    "parameters": tool.inputSchema if tool.inputSchema else {"type": "object", "properties": {}},
                },
            })
        return openai_tools

    async def call_tool(self, name: str, arguments: dict[str, Any]) -> str:
        """è°ƒç”¨ MCP å·¥å…·"""
        if not self.session:
            raise RuntimeError("MCP session not connected")

        result = await self.session.call_tool(name, arguments=arguments)

        # æå–ç»“æœæ–‡æœ¬
        texts = []
        for block in result.content:
            text = getattr(block, "text", None)
            if isinstance(text, str):
                texts.append(text)
            else:
                texts.append(str(block))
        return "\n".join(texts)


class MCPInferencer:
    """MCP æ¨ç†å™¨

    ç»“åˆ LLM å’Œ MCP å·¥å…·è°ƒç”¨ï¼Œå®ç°æ™ºèƒ½åŠ©æ‰‹ã€‚

    Example:
        ```python
        mcp_config = MCPConfig(mcp_url="https://mcp.mcd.cn/mcp-servers/mcd-mcp")
        inferencer = MCPInferencer(mcp_config=mcp_config)
        await inferencer.run()
        ```
    """

    def __init__(
        self,
        mcp_config: MCPConfig,
        llm_config: LLMConfig | None = None,
    ):
        self.mcp_config = mcp_config
        self.llm_config = llm_config or LLMConfig()

        self.mcp_client = MCPClient(mcp_config)
        self.llm_client: AsyncOpenAI | None = None

        self.is_running = False
        self.messages: list[dict[str, Any]] = []

    async def _init_llm_client(self) -> None:
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        self.llm_client = AsyncOpenAI(
            base_url=self.llm_config.base_url,
            api_key=self.llm_config.api_key,
            http_client=httpx.AsyncClient(verify=False),
        )
        print(f"âœ“ LLM client initialized: {self.llm_config.base_url}")

    async def process_user_input(self, user_input: str) -> str:
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè°ƒç”¨ LLM å’Œ MCP å·¥å…·"""
        if not self.llm_client:
            return "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.messages.append({"role": "user", "content": user_input+" /no_think"})
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")

        # è·å– MCP å·¥å…·
        tools = self.mcp_client.get_tools_for_openai()

        # è°ƒç”¨ LLM
        response = await self.llm_client.chat.completions.create(
            model=self.llm_config.model,
            messages=self.messages,
            tools=tools if tools else None,
            stream=True,
        )

        # å¤„ç†æµå¼å“åº”
        content_chunks = []
        tool_calls = []

        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        async for chunk in response:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta is None:
                continue

            if delta.content:
                content_chunks.append(delta.content)
                print(delta.content, end="", flush=True)

            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    while len(tool_calls) <= tool_call.index:
                        tool_calls.append({
                            "id": None,
                            "type": "function",
                            "function": {"name": None, "arguments": ""},
                        })
                    target = tool_calls[tool_call.index]
                    if tool_call.id:
                        target["id"] = tool_call.id
                    if tool_call.function and tool_call.function.name:
                        target["function"]["name"] = tool_call.function.name
                    if tool_call.function and tool_call.function.arguments:
                        target["function"]["arguments"] += tool_call.function.arguments

        print()  # æ¢è¡Œ

        # æ„å»ºåŠ©æ‰‹æ¶ˆæ¯
        assistant_message: dict[str, Any] = {
            "role": "assistant",
            "content": "".join(content_chunks),
        }
        if tool_calls:
            assistant_message["tool_calls"] = tool_calls
        self.messages.append(assistant_message)

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
        if tool_calls:
            for tool_call in tool_calls:
                tool_name = tool_call["function"]["name"]
                tool_args = json.loads(tool_call["function"]["arguments"]) if tool_call["function"]["arguments"] else {}

                print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}({tool_args})")

                try:
                    result = await self.mcp_client.call_tool(tool_name, tool_args)
                    print(f"ğŸ“‹ å·¥å…·ç»“æœ: {result[:200]}..." if len(result) > 200 else f"ğŸ“‹ å·¥å…·ç»“æœ: {result}")
                except Exception as e:
                    result = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
                    print(f"âŒ {result}")

                # æ·»åŠ å·¥å…·ç»“æœ
                self.messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": result,
                })

            # å†æ¬¡è°ƒç”¨ LLM è·å–æœ€ç»ˆå›å¤
            response = await self.llm_client.chat.completions.create(
                model=self.llm_config.model,
                messages=self.messages,
                stream=True,
            )

            final_content = []
            print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
            async for chunk in response:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                if delta and delta.content:
                    final_content.append(delta.content)
                    print(delta.content, end="", flush=True)
            print()

            final_message = {"role": "assistant", "content": "".join(final_content)}
            self.messages.append(final_message)
            return final_message["content"]

        return assistant_message["content"]

    async def run(self) -> None:
        """è¿è¡Œæ–‡æœ¬æ¨¡å¼"""
        print("=" * 50)
        print("ğŸ” éº¦å½“åŠ³ MCP æ™ºèƒ½åŠ©æ‰‹")
        print("=" * 50)

        self.is_running = True

        # åˆå§‹åŒ–
        await self._init_llm_client()
        await self.mcp_client.connect()

        # åˆå§‹åŒ–æ¶ˆæ¯å†å²
        self.messages = [
            {"role": "system", "content": self.llm_config.system_prompt}
        ]

        print("\nğŸ’¬ æ–‡æœ¬æ¨¡å¼å¯åŠ¨")
        print("   è¾“å…¥ 'é€€å‡º' æˆ– 'exit' åœæ­¢\n")

        try:
            while self.is_running:
                try:
                    user_input = await asyncio.get_event_loop().run_in_executor(
                        None, lambda: input("ğŸ‘¤ è¯·è¾“å…¥: ")
                    )

                    if user_input.strip().lower() in ["é€€å‡º", "ç»“æŸ", "exit", "quit"]:
                        print("\nğŸ‘‹ å†è§!")
                        break

                    if not user_input.strip():
                        continue

                    await self.process_user_input(user_input)

                except EOFError:
                    break

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        finally:
            self.is_running = False
            await self.mcp_client.disconnect()
            print("âœ“ MCP è¿æ¥å·²å…³é—­")

    async def __aenter__(self) -> "MCPInferencer":
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        self.is_running = False
        await self.mcp_client.disconnect()


class VoiceMCPEventHandler(RealtimeEventHandler):
    """è¯­éŸ³ MCP äº‹ä»¶å¤„ç†å™¨ - å°†è¯­éŸ³è½¬å†™ç»“æœä¼ é€’ç»™é˜Ÿåˆ—"""

    def __init__(self, transcript_queue: asyncio.Queue):
        self.transcript_queue = transcript_queue

    async def on_session_created(self, session_id: str) -> None:
        print(f"âœ“ Voice session created: {session_id}")

    async def on_session_updated(self) -> None:
        print("âœ“ Voice session updated")

    async def on_transcript_delta(self, event: TranscriptEvent) -> None:
        """æ¯ä¸ª delta å°±æ˜¯ä¸€æ®µå®Œæ•´çš„è½¬å†™ç»“æœ"""
        print(f"\nğŸ¤ è¯­éŸ³è¯†åˆ«: {event.delta}")
        if event.delta.strip():
            await self.transcript_queue.put(event.delta.strip())

    async def on_audio_delta(self, event: AudioEvent) -> None:
        pass  # ä¸æ’­æ”¾éŸ³é¢‘

    async def on_response_done(self) -> None:
        pass

    async def on_input_transcript_completed(self, event: InputTranscriptEvent) -> None:
        pass  # ä¸ä½¿ç”¨è¿™ä¸ªäº‹ä»¶


class VoiceMCPInferencer:
    """è¯­éŸ³ + MCP æ¨ç†å™¨

    ç»“åˆå®æ—¶è¯­éŸ³è¯†åˆ«å’Œ MCP å·¥å…·è°ƒç”¨ï¼Œå®ç°è¯­éŸ³æ§åˆ¶çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

    Example:
        ```python
        mcp_config = MCPConfig(mcp_url="https://mcp.mcd.cn/mcp-servers/mcd-mcp")
        inferencer = VoiceMCPInferencer(mcp_config=mcp_config)
        await inferencer.run()
        ```
    """

    def __init__(
        self,
        mcp_config: MCPConfig,
        llm_config: LLMConfig | None = None,
        realtime_config: RealtimeConfig | None = None,
    ):
        self.mcp_config = mcp_config
        self.llm_config = llm_config or LLMConfig()
        self.realtime_config = realtime_config or RealtimeConfig()

        self.mcp_inferencer = MCPInferencer(mcp_config, llm_config)
        self.realtime_inferencer: RealtimeInferencer | None = None

        self.is_running = False
        self._transcript_queue: asyncio.Queue[str] = asyncio.Queue()

    async def _process_transcripts(self) -> None:
        """å¤„ç†è¯­éŸ³è½¬å†™ç»“æœé˜Ÿåˆ—"""
        while self.is_running:
            try:
                # ç­‰å¾…è¯­éŸ³è¾“å…¥
                transcript = await asyncio.wait_for(
                    self._transcript_queue.get(), timeout=0.5
                )

                # æ£€æŸ¥æ˜¯å¦é€€å‡º
                if transcript in ["é€€å‡º", "ç»“æŸ", "åœæ­¢", "exit", "quit"]:
                    print("\nğŸ‘‹ æ”¶åˆ°é€€å‡ºæŒ‡ä»¤ï¼Œæ­£åœ¨åœæ­¢...")
                    self.is_running = False
                    break

                # å¤„ç†è¯­éŸ³è¾“å…¥
                await self.mcp_inferencer.process_user_input(transcript)

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"\nâŒ å¤„ç†è½¬å†™ç»“æœå‡ºé”™: {e}")

    async def run(self) -> None:
        """è¿è¡Œè¯­éŸ³ MCP æ¨¡å¼"""
        print("=" * 50)
        print("ğŸ” éº¦å½“åŠ³ MCP æ™ºèƒ½åŠ©æ‰‹ (è¯­éŸ³æ¨¡å¼)")
        print("=" * 50)

        self.is_running = True

        # åˆå§‹åŒ– MCP æ¨ç†å™¨
        await self.mcp_inferencer._init_llm_client()
        await self.mcp_inferencer.mcp_client.connect()
        self.mcp_inferencer.messages = [
            {"role": "system", "content": self.mcp_inferencer.llm_config.system_prompt}
        ]
        self.mcp_inferencer.is_running = True

        print("\nğŸ¤ è¯­éŸ³æ¨¡å¼å¯åŠ¨ï¼Œè¯·è¯´è¯...")
        print("   è¯´ 'é€€å‡º' æˆ– 'ç»“æŸ' åœæ­¢\n")

        # åˆ›å»ºäº‹ä»¶å¤„ç†å™¨
        event_handler = VoiceMCPEventHandler(self._transcript_queue)

        # åˆ›å»ºå®æ—¶æ¨ç†å™¨
        self.realtime_inferencer = RealtimeInferencer(
            config=self.realtime_config,
            event_handler=event_handler,
        )

        try:
            # å¹¶è¡Œè¿è¡Œè¯­éŸ³è¯†åˆ«å’Œè½¬å†™å¤„ç†
            await asyncio.gather(
                self.realtime_inferencer.run(enable_audio_playback=False),
                self._process_transcripts(),
            )
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        finally:
            self.is_running = False
            if self.realtime_inferencer:
                await self.realtime_inferencer.stop()
            await self.mcp_inferencer.mcp_client.disconnect()
            print("âœ“ æ‰€æœ‰è¿æ¥å·²å…³é—­")

    async def __aenter__(self) -> "VoiceMCPInferencer":
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        self.is_running = False
        if self.realtime_inferencer:
            await self.realtime_inferencer.stop()
        await self.mcp_inferencer.mcp_client.disconnect()

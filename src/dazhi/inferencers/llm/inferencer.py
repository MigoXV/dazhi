"""LLM æŽ¨ç†å™¨æ¨¡å— - ä»…è´Ÿè´£ LLM å¯¹è¯æŽ¨ç†"""

import asyncio
from typing import Any

import httpx
from openai import AsyncOpenAI

from dazhi.inferencers.llm.session import LLMChatSession
from dazhi.mcp_adaptors.config import LLMConfig


class LLMInferencer:
    """LLM æŽ¨ç†å™¨

    ä»…è´Ÿè´£ LLM å¯¹è¯æŽ¨ç†ï¼Œä¸æ¶‰åŠ MCP å·¥å…·è°ƒç”¨ã€‚

    Example:
        ```python
        llm_config = LLMConfig()
        inferencer = LLMInferencer(llm_config=llm_config)
        await inferencer.run()
        ```
    """

    def __init__(
        self,
        llm_config: LLMConfig,
    ):
        self.llm_config = llm_config
        self.llm_client: AsyncOpenAI | None = None
        self.is_running = False

    async def _init_llm_client(self) -> None:
        """åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        self.llm_client = AsyncOpenAI(
            base_url=self.llm_config.base_url,
            api_key=self.llm_config.api_key,
            http_client=httpx.AsyncClient(verify=False),
        )
        print(f"âœ“ LLM client initialized: {self.llm_config.base_url}")

    async def process_user_input(
        self,
        session: LLMChatSession,
        user_input: str,
        tools: list[dict[str, Any]] | None = None,
    ) -> dict[str, Any]:
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œä»…è°ƒç”¨ LLM æŽ¨ç†"""
        if not self.llm_client:
            return {"role": "assistant", "content": "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        session.add_user(user_input)
        print(f"\nðŸ‘¤ ç”¨æˆ·: {user_input}")

        assistant_message = await self._stream_completion(session=session, tools=tools)
        session.add_assistant(assistant_message)
        return assistant_message

    async def continue_assistant(self, session: LLMChatSession) -> dict[str, Any]:
        """ç»§ç»­åŸºäºŽå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆå›žå¤"""
        if not self.llm_client:
            return {"role": "assistant", "content": "LLM å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}

        assistant_message = await self._stream_completion(session=session)
        session.add_assistant(assistant_message)
        return assistant_message

    async def _stream_completion(
        self,
        session: LLMChatSession,
        tools: list[dict[str, Any]] | None = None,
    ) -> dict[str, Any]:
        response = await self.llm_client.chat.completions.create(
            model=self.llm_config.model,
            messages=session.messages,
            tools=tools if tools else None,
            stream=True,
        )

        content_chunks = []
        tool_calls = []

        print("ðŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        async for chunk in response:
            if not chunk.choices:
                continue
            delta = chunk.choices[0].delta
            if delta is None:
                continue

            if delta.content:
                content_chunks.append(delta.content)
                print(delta.content, end="", flush=True)

            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    while len(tool_calls) <= tool_call.index:
                        tool_calls.append(
                            {
                                "id": None,
                                "type": "function",
                                "function": {"name": None, "arguments": ""},
                            }
                        )
                    target = tool_calls[tool_call.index]
                    if tool_call.id:
                        target["id"] = tool_call.id
                    if tool_call.function and tool_call.function.name:
                        target["function"]["name"] = tool_call.function.name
                    if tool_call.function and tool_call.function.arguments:
                        target["function"]["arguments"] += tool_call.function.arguments

        print()

        assistant_message: dict[str, Any] = {
            "role": "assistant",
            "content": "".join(content_chunks),
        }
        if tool_calls:
            assistant_message["tool_calls"] = tool_calls
        return assistant_message

    async def run(self) -> None:
        """è¿è¡Œæ–‡æœ¬æ¨¡å¼"""
        print("=" * 50)
        print("ðŸ” LLM æ™ºèƒ½åŠ©æ‰‹")
        print("=" * 50)

        self.is_running = True

        # åˆå§‹åŒ–
        await self._init_llm_client()

        # åˆå§‹åŒ–ä¼šè¯
        session = LLMChatSession(system_prompt=self.llm_config.system_prompt)

        print("\nðŸ’¬ æ–‡æœ¬æ¨¡å¼å¯åŠ¨")
        print("   è¾“å…¥ 'é€€å‡º' æˆ– 'exit' åœæ­¢\n")

        try:
            while self.is_running:
                try:
                    user_input = await asyncio.get_event_loop().run_in_executor(
                        None, lambda: input("ðŸ‘¤ è¯·è¾“å…¥: ")
                    )

                    if user_input.strip().lower() in ["é€€å‡º", "ç»“æŸ", "exit", "quit"]:
                        print("\nðŸ‘‹ å†è§!")
                        break

                    if not user_input.strip():
                        continue

                    await self.process_user_input(session, user_input)

                except EOFError:
                    break

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
        finally:
            self.is_running = False

    async def __aenter__(self) -> "LLMInferencer":
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        self.is_running = False
